<?xml version="1.0" encoding="UTF-8"?>
<!--Generated by Site-Server v6.0.0-3718-3718 (http://www.squarespace.com) on Mon, 23 Mar 2015 16:57:30 GMT
--><rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://www.rssboard.org/media-rss" version="2.0"><channel><title>Blog - Research Repository</title><link>http://andrew-young-513z.squarespace.com/blog/</link><lastBuildDate>Mon, 23 Mar 2015 16:47:32 +0000</lastBuildDate><language>en-US</language><generator>Site-Server v6.0.0-3718-3718 (http://www.squarespace.com)</generator><description></description><item><title>Bridging the Knowledge Gap: In Search of Expertise</title><category>Blog</category><dc:creator>Andrew Young</dc:creator><pubDate>Mon, 23 Mar 2015 16:46:02 +0000</pubDate><link>http://andrew-young-513z.squarespace.com/blog/2015/3/23/bridging-the-knowledge-gap-in-search-of-expertise</link><guid isPermaLink="false">54ef86f0e4b060284dcf9f23:54ef8cdce4b0819f8ee6fa11:551042d4e4b01eb65290d925</guid><description><![CDATA[<p><em>By Beth Simone Noveck</em></p><p><span><span>In the early 2000s, the Air Force struggled with a problem: Pilots and civilians were dying because of unusual soil and dirt conditions in Afghanistan. The soil was getting into the rotors of the Sikorsky UH-60 helicopters and obscuring the view of its pilots—what the military calls a “brownout.” According to the Air Force’s senior design scientist, the manager tasked with solving the problem didn’t know where to turn quickly to get help. As it turns out, the man practically sitting across from him had nine years of experience flying these Black Hawk helicopters in the field, but the manager had no way of knowing that. Civil service titles such as director and assistant director reveal little about skills or experience.</span></span></p><p><span><span>In the fall of 2008, the Air Force sought to fill in these kinds of knowledge gaps. The Air Force Research Laboratory unveiled Aristotle, a searchable internal directory that integrated people’s credentials and experience from existing personnel systems, public databases, and users themselves, thus making it easy to discover quickly who knew and had done what. Near-term budgetary constraints killed Aristotle in 2013, but the project underscored a glaring need in the bureaucracy.</span></span></p><p><span><span>Aristotle was an attempt to solve a challenge faced by every agency and organization: quickly locating expertise to solve a problem. Prior to Aristotle, the DOD had no coordinated mechanism for identifying expertise across 200,000 of its employees. Dr. Alok Das, the senior scientist for design innovation tasked with implementing the system, explained, “We don’t know what we know.”</span></span></p><p><span><span>This is a common situation. The government currently has no systematic way of getting help from all those with relevant expertise, experience, and passion. For every success on Challenge.gov—the federal government’s platform where agencies post open calls to solve problems for a prize—there are a dozen open-call projects that never get seen by those who might have the insight or experience to help. This kind of crowdsourcing is still too ad hoc, infrequent, and unpredictable—in short, too unreliable—for the purposes of policy-making.</span></span></p><p><span><span>Which is why technologies like Aristotle are so exciting. Smart, searchable expert networks offer the potential to lower the costs and speed up the process of finding relevant expertise. Aristotle never reached this stage, but an ideal expert network is a directory capable of including not just experts within the government, but also outside citizens with specialized knowledge. This leads to a dual benefit: accelerating the path to innovative and effective solutions to hard problems while at the same time fostering greater citizen engagement.</span></span></p><p><span><span>Could such an expert-network platform revitalize the regulatory-review process? We might find out soon enough, thanks to the Food and Drug Administration. The FDA is working on a pilot project called FDA Profiles that aims to revolutionize the way regulators perform their duties by enabling them to find and target people with the right expertise to advise them. And a survey of how the FDA does its work now suggests that reform is sorely needed.</span></span></p><p><span><span>Among its responsibilities, the FDA is tasked with reviewing the efficacy and safety of new medical devices, which include a range of products, from pregnancy tests to heart stents, that reduce suffering, extend lives, treat diseases, and generate enormous economic gain for successful inventors. The Center for Devices and Radiological Health (CDRH), a division of the FDA, manages this complex process of pre-market approval and post-market review of all medical devices. Within CDRH, still another agency, called the Office of Science and Engineering Laboratories (OSEL), employs its own scientists and consults with outside experts whose job it is to understand the safety and effectiveness of medical devices from invention to eventual use.</span></span></p><p><span><span>The pathway to regulatory review and compliance for low-risk items like tongue depressors is straightforward. But life-sustaining or high-risk devices such as pacemakers and breast implants require judicious and timely pre-market approval by those with the right know-how. If these medical devices reach patients prior to being properly tested, there may well be very real human costs. Dr. Steven Nissen of the Cleveland Clinic estimates that faulty devices contributed to more than 2,800 deaths in the year 2006 alone. In a CBS News report on medical implants, Dr. Nissen affirmed, “People make the assumption that when their doctor implants a device, whether it be an artificial joint or a pacemaker, that it’s undergone very rigorous testing. That assumption isn’t always true.”</span></span></p><p><span><span>In the current model of pre-market review, several challenges exist. For one thing, the process can take too long. At present, it can take nine months just to find and convene a qualified review panel. There’s a reason for this. Only 100 scientists work at OSEL—800 total at CDRH—and there’s always a need for more expertise. Staff at CDRH also have to oversee the safety of devices throughout their life cycle, creating further burdens. This partly explains why, from 2000 to 2010, the average time to decision—the time from receipt of an application to a determination—on high-risk but potentially life-saving devices increased by nearly 60 percent, from 96 to 153 days. Over approximately the same time period, the number of submissions for pre-market approval remained the same, hovering just below 10,000 per year. Simply put, the FDA needs to get faster at its job, but doesn’t have the budget to bolster its ranks. Nor will it in the near future.</span></span></p><p><span><span>Another problem is that the people who are already in the agency might not be the experts the agency needs most. In January 2011, the FDA committed to a plan known as the Innovation Pathway to transform its regulatory process and improve how it works with entrepreneurs, both to bring new devices to the market and to protect the public. The Innovation Pathway pinpointed this expertise deficit as a key problem, citing the need for greater collaboration with outside experts. There are many reasons for the lack of expertise: high reviewer and manager turnover at CDRH (almost double that of FDA’s drug and biologics centers), insufficient training for staff, extremely high ratios of frontline supervisors to employees, and a rapidly growing workload. The Innovation Pathway undertook a pilot project to create a vetted list of experts across two dozen membership organizations, such as the American Academy of Neurology and the Society of Thoracic Surgeons. In the absence of any means to “match” people to problems with any specificity, it relies on membership in these professional associations to serve as a proxy for expertise. But while membership in the Society of Thoracic Surgeons might require that one practice as a thoracic surgeon, it doesn’t tell the agency much beyond that. Consequently, the list is fairly limited in its ability to pinpoint expertise on the basis of experience or interest. Moreover, the vetted list has done a poor job of reaching those who possess a great deal of know-how but don’t belong to one these professional associations: A doctoral candidate at a university whose dissertation is on a specific medical device, for example, would not be captured by its net.</span></span></p><p><span><span>A third issue in regulatory review is diversity. The nature of the process demands access to a diversity of expertise. There is a wide variety of devices up for review (a 3-D printed exoskeleton is very different from a heart-rate monitoring app), and they are often cutting-edge, necessitating new kinds of expertise from many different fields, such as materials science, electrical engineering, and fluid mechanics. Indeed, they are sometimes hard to describe and are often called by different names in different fields, which makes it even more challenging to get help. In addition, there is always the need for insight from people in fields where one wouldn’t assume any prior knowledge. For example, it may be an entomologist who studies the architecture of anthills in the African savanna who might know the most about green building design. It might be the poet or the philosopher who has something to share.</span></span></p><p><span><span>Finally, the review process faces a sophistication problem. A study completed by the Boston Consulting Group points to a significant rise in the number of drugs and complex devices approved in the EU long before the United States. As devices get more complex, the United States is likely to fall further behind. Although the FDA does not issue clear data on backlogs or processing times, research points to shortcomings in our current system when it comes to researching and approving complex devices. This has broad implications for the quality of health care that Americans receive. In 2010, for example, the medical device company Biosensors International shut down its operations in California due to the time and expense associated with getting FDA approval for a cardiac stent. That device is available globally, including in Mexico and Canada.</span></span></p><p><span><span>New expert-network platforms offer the promise of overcoming these problems and decreasing the cost of finding expertise. This is especially true in highly technical fields with a critical mass of academics and regulators, such as biotechnology and software technology. Academics tend to maintain accurate and up-to-date data about their credentials and publications. Regulators are far less transparent about their credentials but more so about their experience, at least internally—there is a record of which regulator worked on which regulatory action. In short, these two groups—academics and regulators—already collect data that can help in creating expert networks and automating requests to participate in the review process.</span></span></p><p><span><span>There is no shortage of knowledgeable people in the biomedical sciences. In addition to the 800 scientists at CDRH, approximately 14,000 people work at the wider FDA, while another 6,000 scientists are part of a broader population of 17,000 at NIH, and more than 83,000 at their parent agency, Health and Human Services—not to mention the hundreds of thousands of biomedical professionals working in industry and academia. But the FDA has no way to quickly discover which of these individuals possess relevant expertise to review device applications. In order to enhance its research and regulatory activities and reduce the time needed for review, the FDA needs to be able to find experts to participate in review panels, as well as to educate the agency about new trends in medical devices.</span></span></p><p><span><span>Enter FDA Profiles. Funded by the NIH, Profiles is an open-source expert discovery and networking tool that aids in finding researchers with specific areas of expertise. It imports “white pages” information, publications, and other data sources and assembles a searchable library of electronic CVs. Although unique in government, Profiles is not revolutionary—it doesn’t upset the apple cart of power and change who makes the ultimate decisions at the FDA about devices. Its goal is to provide regulators with better and more relevant information more quickly. Profiles uses a platform developed by Harvard Medical School with support from the NIH that is in use at the biomedical faculties of 240 institutions, including Harvard, Penn State, Boston University, and University of California, San Francisco.</span></span></p><p><span><span>Slated to launch later this year, the FDA Profiles pilot project will create an online, searchable expert directory, initially of FDA staff in the first phase and later of outside scientists. Its expertise algorithms are designed to add publications automatically to a person’s Profiles bio from online sources and then to analyze these data to build a catalog of people’s expertise and their research network. Networks are automatically created based on current or past co-authorship history, organizational relationships, and geographic proximity. The system can also pull data from online biographies, professional society memberships, education and training records, affiliations, regulatory accomplishments, publications drawn from open databases, and grants culled from Grants.gov to auto-populate a profile. In addition, a researcher can manually add data about his or her interests, skills, and projects.</span></span></p><p><span><span>With both passively gleaned and actively contributed data, Profiles is intended to speed the process of locating the “right” experts by algorithmically matching people to opportunities to participate in the review process. For instance: Type in “stent” and FDA Profiles will list the relevant people at the FDA, with explanations for why they were suggested for inclusion in a panel. Although Profiles will initially function as a kind of “FDA LinkedIn,” the plan is to connect with other networks to find expertise outside the agency. In the future, there are ambitious plans to incorporate patents as well as data from LinkedIn, SlideShare, and other social media.</span></span></p><p><span><span>The FDA Profiles project will allow regulators to gather empirical evidence about whether targeting expertise helps the agency overcome the challenges in the regulatory-review process. The FDA needs expertise at different stages of decision-making, from identifying emerging trends to reviewing a specific device. FDA Profiles affords a chance to study whether expert networking and targeting participation by those with specific kinds of expertise work better at different stages of regulatory review. What we stand to learn from Profiles is whether it leads to faster, more comprehensive, and more effective review practices that get safe devices to the market faster and, ultimately, improve people’s lives by spotting problems sooner. FDA Profiles can also tell us the impact of expert networking on crowdsourcing more generally, and whether targeting participation helps to improve the process of working collaboratively across an agency or department and with the broader public. But drawing conclusions will require adding testing into the process, including randomized controlled trials, such as comparing what happens when the agency issues a general call to participate in a regulatory-review panel versus populating such a panel using Profiles. Doing so will advance our understanding of how to introduce approaches to policy-making that are both empirically validated yet agile in design.</span></span></p><p><span><span>The role of the regulator is fraught. Regulatory agencies are frequently under siege from industry groups that bemoan burdensome and costly requirements. At the same time, consumer groups assault them for failing to prioritize the public interest and protect citizens from dangerous and fraudulent products and services. Critics on both left and right challenge regulators’ ability to impose requirements and complain about their inefficiency and ineffectiveness. In the context of device review, grant review, and other domains where expertise is clearly called for, using expert-network platforms can only democratize what are now comparatively closed processes that typically rely on the same people to participate. Although the data from the United States are anecdotal, data from Europe show that the same repeat players serve on multiple scientific advisory committees again and again.</span></span></p><p><span><span>New technology has opened up possibilities for changing how we make decisions that could improve both the effectiveness and legitimacy of regulation. FDA Profiles could change the default on how the agency makes decisions by bringing in diverse expertise at the outset, and has the potential to do more once it begins to get used outside the FDA. The hope is that by finding the right experts and matching them to opportunities to participate, the agency can apply their knowledge toward better and faster decision-making. And the FDA’s willingness to experiment with projects like FDA Profiles potentially heralds a sea change in how government agencies use empirical research to drive institutional innovation. When social-science research is integrated with the practice of policy-making, we can gain insight into our decision-making apparatus and apply what we learn to the broader goal of improving governance.</span></span></p><p><span><span>As President Obama said in his 2009 Memorandum on Transparency and Open Government, “[K]nowledge is widely dispersed in society, and public officials benefit from having access to that dispersed knowledge” and, hence, to collective expertise and wisdom. There is no agency or topic that would not benefit from the ability to match people with relevant expertise to the opportunity to participate in the regulatory-review process. It’s long overdue to start using the tools available to us to inform how agencies make such important decisions</span></span></p>]]></description></item><item><title>Stefaan Verhulst on the GovLab's Distributed Internet Governance Project</title><category>Video</category><dc:creator>Andrew Young</dc:creator><pubDate>Thu, 26 Feb 2015 21:19:23 +0000</pubDate><link>http://andrew-young-513z.squarespace.com/blog/2015/2/26/research-talk-stefaan-verhulst-on-the-govlabs-distributed-internet-governance-project</link><guid isPermaLink="false">54ef86f0e4b060284dcf9f23:54ef8cdce4b0819f8ee6fa11:54ef8dcce4b0977b58117e21</guid><description><![CDATA[
	
	
		
			
				
					<img class="thumb-image" alt="Screen Shot 2015-03-18 at 8.32.14 PM.png" data-image="http://static1.squarespace.com/static/54ef86f0e4b060284dcf9f23/t/550a1927e4b065983346f205/1426725163622/Screen+Shot+2015-03-18+at+8.32.14+PM.png" data-image-dimensions="1422x794" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="550a1927e4b065983346f205" data-type="image" src="http://static1.squarespace.com/static/54ef86f0e4b060284dcf9f23/t/550a1927e4b065983346f205/1426725163622/Screen+Shot+2015-03-18+at+8.32.14+PM.png?format=500w" />
				
			

			

		
	
	
<p><strong>Project:&nbsp;</strong>NETmundial Solutions Map</p><p><strong>Lead</strong>: Stefaan Verhulst, The GovLab</p><p><strong>Tags</strong></p><p><em>Innovation</em>: Crowdsourcing, Distributed Governance</p><p><em>Objective:&nbsp;</em>Participation</p><p><em>Sector</em>: Internet</p><p><em>Methodology: </em>Conceptual Framework<em>,&nbsp;</em>Survey</p>]]></description></item><item><title>Measuring the Impact of Public Innovation in the Wild</title><category>Blog</category><dc:creator>Andrew Young</dc:creator><pubDate>Thu, 26 Feb 2015 21:17:39 +0000</pubDate><link>http://andrew-young-513z.squarespace.com/blog/2015/2/26/building-research-into-crowdsourced-civic-mapping-initiatives</link><guid isPermaLink="false">54ef86f0e4b060284dcf9f23:54ef8cdce4b0819f8ee6fa11:54ef8d50e4b050234b7351ec</guid><description><![CDATA[<p><em>By Beth Simone Noveck</em></p><p><span><span>With complex, seemingly intractable problems such as inequality, climate change and affordable access to health care plaguing contemporary society, traditional institutions such as government agencies and nonprofit organizations often lack strategies for tackling them effectively and legitimately. For this reason, this year the MacArthur Foundation launched its </span><a href="http://www.opening-governance.org/"><span>Research Network on Opening Governance</span></a><span>.</span></span></p><p><span><span>The Network, which I chair and which also is supported by Google.org, is what MacArthur calls a "research institution without walls." It brings together </span><a href="http://www.opening-governance.org/members"><span>a dozen researchers across universities and disciplines</span></a><span>, with an advisory network of academics, technologists, and current and former government officials, to study new ways of addressing public problems using advances in science and technology.</span></span></p><p><span><span>Through regular meetings and collaborative projects, the Network is exploring, for example, the latest techniques for more open and transparent decision-making, the uses of data to transform how we govern, and the identification of an individual's skills and experiences to improve collaborative problem-solving between government and citizen.</span></span></p><p><span><span>One of the central questions we are grappling with is how to accelerate the pace of research so we can learn better and faster when an innovation in governance works -- for whom, in which contexts and under which conditions. With better methods for doing fast-cycle research in collaboration with government -- in the wild, not in the lab -- our hope is to be able to predict with accuracy, not just know after the fact, whether innovations such as opening up an agency's data or consulting with citizens using a crowdsourcing platform are likely to result in real improvements in people's lives.</span></span></p><p><span><span>An example of such an experiment is the work that members of the Network are undertaking with the Food and Drug Administration. As one of its duties, the FDA manages the process of pre-market approval of medical devices to ensure that patients and providers have timely access to safe, effective and high-quality technology, as well as the post-market review of medical devices to ensure that unsafe ones are identified and recalled from the market. In both of these contexts, the FDA seeks to provide the medical-device industry with productive, consistent, transparent and efficient regulatory pathways.</span></span></p><p><span><span>With thousands of devices, many of them employing cutting-edge technology, to examine each year, the FDA is faced with the challenge of finding the right internal and external expertise to help it quickly study a device's safety and efficacy. Done right, lives can be saved and companies can prosper from bringing innovations quickly to market. Done wrong, bad devices can kill.</span></span></p><p><span><span>Beginning in 2015, the FDA will start to use an expert network that catalogs and makes the expertise of its employees searchable, as well as the expertise of its parent Department of Health and Human Services, the broader academic community and industry. Network members with expertise in policy, law, computer science, public health and behavioral economics are working with the FDA to design and deploy experiments to test the capacity of such expert networking to improve the medical device review process over a six-month time frame.</span></span></p><p><span><span>Improving the efficacy of medical-device review is just an initial foray for the Network. If we can generate rapid results and insights into what's working across government, our hope is to develop a replicable, step-by-step approach to studying governance innovations in the wild -- documenting the underlying change theory, for example, as well as producing a set of metrics to track performance. Done right, such research could help innovators inside and outside of government mitigate risk and try new ways of working, safe in the knowledge that they will have some evidence of what works.</span></span></p><p><span><em>Government readers interested in partnering with the MacArthur Research Network should contact </em><em>Andrew@thegovlab.org</em><span>.</span></span></p>]]></description></item><item><title>A “calculus” for open data</title><category>Blog</category><dc:creator>Andrew Young</dc:creator><pubDate>Thu, 26 Feb 2015 21:16:50 +0000</pubDate><link>http://andrew-young-513z.squarespace.com/blog/2015/2/26/methodology-insights-from-the-academy-of-management-2015-annual-meeting</link><guid isPermaLink="false">54ef86f0e4b060284dcf9f23:54ef8cdce4b0819f8ee6fa11:54ef8d09e4b0d9f79524f76c</guid><description><![CDATA[<p><em>By Arnaud Sahuguet and David Sangokoya</em></p><p><span><span>Open data makes great promises and offers untapped benefits for individual, public and private decision-making. However, these benefits often come with hidden costs and risks. Taking some inspiration from </span><a href="http://journals.cambridge.org/abstract_S0003055400000125"><span>“</span><em>A theory of the calculus of voting”</em></a><span>, we present a modest attempt at formalizing a “calculus of open data” to help data providers make this decision.</span></span></p><p><span><strong>Introduction</strong></span></p><p><span><span>The value, impact and promise of making data publicly accessible have driven citizens, government agencies and businesses to embrace open data as a way to increase efficiency, promote transparency and maximize utility.</span></span></p><p><span><em>Open data is data that can be freely used, reused and redistributed by anyone — subject only, at most, to the requirement to attribute and share-alike. [</em><a href="http://opendatahandbook.org/en/what-is-open-data/"><em>Open Data Handbook</em></a><em>]</em></span></p><p><span><span>McKinsey estimates more than $3 trillion in additional value globally as a result of open data. Large scale studies such as the </span><a href="http://www.opendata500.com/"><span>Open Data 50</span></a><span>0 reveal an impact across sectors such as energy, consumer products and healthcare. More than 40 countries have shared over a million government datasets. Shared corporate data provides mutual benefits for both public and private sector entities, e.g. Uber’s </span><a href="http://blog.uber.com/city-data"><span>partnership</span></a><span> with the City of Boston, the Twitter-MIT </span><a href="http://socialmachines.media.mit.edu/"><span>Laboratory for Social Machines</span></a><span> and </span><a href="http://www.yelp.com/dataset_challenge"><span>Yelp’s Dataset Challenge</span></a><span>.</span></span></p><p><span><span>While the rise of the open data movement has led to numerous commitments and increasing enthusiasm to unleash the potential of open data, data providers lack a common language for evaluating and weighing the decision to open their own data.</span></span></p><p><span><span>Government agencies and city officials often open their data as a result of top-down pressure to champion efficiency, meet citizen demand or increase transparency through the number of datasets released rather than the impact these datasets may create. They often do not understand the hidden costs associated with opening their data and miss opportunities to leverage knowledge from communities or outside expertise for optimal data sharing.</span></span></p><p><span><span>Corporations</span><strong> </strong><span>are mostly embracing a wait-and-see attitude. While some have begun sharing corporate data for research or policymaking intended for public benefit, others are building business models from public open data. Since data is seen as a business asset holding significant value, companies are cautiously considering why they should take on risks to competition and engage in activity amidst nascent legal and regulatory frameworks.</span></span></p><p><span><span>End users</span><strong> </strong><span>are motivated to share their data. However, they often are not the true “owners” — their data being stored and managed on their behalf by technology and social media companies. Even when they are, the fear of unwanted government surveillance or corporate marketing practices dissuades them from making their data more publicly accessible.</span></span></p><p><span><strong>Stories from the field</strong></span></p><p><span><span>We briefly start with a few selected examples to highlight the value and impact of open data and the need for a better decision framework when choosing to open data.</span></span></p><p><span><em>Success and horror stories</em></span></p><p><span><span>We’ve seen the success stories and numerous examples of open data. Public transit information (made available by cities via </span><a href="https://developers.google.com/transit/gtfs/reference"><span>Google GTFS standard</span></a><span>) is saving time to millions of people on a daily basis. GPS data is at the core of mobile location-based services and products. Weather information from the National Oceanic and Atmospheric Administration (NOAA) is used by weather companies and insurance agents such as </span><a href="http://www.weather.com/"><span>The Weather Channel</span></a><span> and the </span><a href="http://www.climate.com/"><span>Climate Corporation</span></a><span>. The open nature of the data in the </span><a href="http://www.genome.gov/10001772"><span>Human Genome Project</span></a><span> fostered at-scale collaboration in decoding the genome and creating an ecosystem of innovation among academic and private researchers.</span></span></p><p><span><span>We’ve also witnessed the horror stories. A seminal example was the release of search logs by AOL for academic research purposes in 2006. The released data contained some publicly-identifiable information (PII) on AOL members that made it possible to identify people and reveal their internet search histories. More recently, the </span><a href="http://chriswhong.com/open-data/foil_nyc_taxi/"><span>release of improperly anonymized taxi data</span></a><span> from the New York City Taxi &amp; Limo Commission revealed the identities of taxi drivers, </span><a href="http://www.fastcompany.com/3036573/fast-feed/nyc-taxi-data-blunder-reveals-which-celebs-dont-tip-and-who-frequents-strip-clubs"><span>trips of celebrities</span></a><span> and even the </span><a href="http://theiii.org/index.php/997/using-nyc-taxi-data-to-identify-muslim-taxi-drivers/"><span>religious orientation of some drivers</span></a><span>.</span></span></p><p><span><strong>The hard questions</strong></span></p><p><span><span>For all these cases, here are some questions that are hard to answer today:</span></span></p><ul><li><p><span><span>why did stakeholders choose to open (or not open) their data?</span></span></p></li><li><p><span><span>what incentives could have been put in place to encourage (or discourage) opening and sharing of data?</span></span></p></li><li><p><span><span>among the various levers one can pull, which one makes the most sense for the data provider?</span></span></p></li></ul><p><span><strong>A calculus for open data</strong></span></p><p><span><span>Our calculus centers around a simple equation:</span></span></p><p class="text-align-center"><strong><span><em>P × B + D &gt; C</em></span></strong></p><p><span><span>where</span></span></p><ul><li><p><span><span>P is the probability that opening the data will have some effect,</span></span></p></li><li><p><span><span>B is the individual benefit of opening the data,</span></span></p></li><li><p><span><span>D is the global or ecosystem impact, and</span></span></p></li><li><p><span><span>C is the cost.</span></span></p></li></ul><p><span><span>Any increase in P, B or D and a decrease in C will make the outcome of opening the data better.</span></span></p><p><span><span>Let's now revisit each variable one by one and look at concrete factors that influence it.</span></span></p><p><span><em>P for probability</em></span></p><p><span><span>(P) represents the probability that opening the data will generate potential benefits for the data owner.</span></span></p><p><span><span>Factors that make (P) go up include:</span></span></p><ul><li><p><span><strong>standards</strong><span> to publish the data</span></span></p></li><li><p><span><span>a </span><strong>data-driven culture</strong><span> inside public and private sectors, fostered by strong education offerings.</span></span></p></li><li><p><span><span>an </span><strong>ecosystem</strong><span> of data consumers, with a hackers/developers to build products, clearinghouses to store and curate data (e.g. </span><a href="http://enigma.io/"><span>Enigma</span></a><span>), data science shops (e.g. </span><a href="http://www.kaggle.com/"><span>Kaggle</span></a><span>, </span><a href="http://www.datakind.org/"><span>DataKind</span></a><span>, </span><a href="http://www.bayesimpact.org/"><span>Bayes Impact</span></a><span>).</span></span></p></li><li><p><span><strong>incentives </strong><span>for consumers to use the data, e.g. prizes &amp; challenges (</span><a href="http://nycbigapps.com/"><span>NYC BigApps</span></a><span>, </span><a href="http://www.netflixprize.com/"><span>Netflix Prize</span></a><span>) or funded research (e.g. </span><a href="https://blog.twitter.com/2014/introducing-twitter-data-grants"><span>Twitter Data Grants</span></a><span>).</span></span></p></li></ul><p><span><span>Factors that make (P) go down include:</span></span></p><ul><li><p><span><span>the </span><strong>absence or inflexibility of legal frameworks</strong><span>, e.g. rigid or non-existent legal framework around data</span></span></p></li><li><p><span><span>the </span><strong>lack of trust</strong><span> among the various players.</span></span></p></li></ul><p><span><em>B for benefits</em></span></p><p><span><span>The potential benefits (B) for the entity opening data include benefits about the quality improvement for the data after being released:</span></span></p><ul><li><p><span><span>better </span><strong>accuracy </strong><span>and less errors as a result of public scrutinization of the data</span></span></p></li><li><p><span><span>less gaps in the data in terms of </span><strong>coverage </strong><span>&amp; </span><strong>granularity </strong><span>coming from external contribution</span></span></p></li><li><p><span><span>better </span><strong>interoperability </strong><span>as a result of un-siloed data</span></span></p></li><li><p><span><strong>sustainability </strong><span>of data</span></span></p></li><li><p><span><span>data </span><strong>prioritization</strong><span>, to help identity most impactful datasets</span></span></p></li><li><p><span><span>improved </span><strong>data collection </strong><span>by other public institutions (decreasing unnecessary duplication and associated costs)</span></span></p></li></ul><p><span><span>Interesting findings about the data may be valuable and result in </span><strong>increased political, social and economic benefits</strong><span> for the data owner, including:</span></span></p><ul><li><p><span><span>development of new products and services</span></span></p></li><li><p><span><span>creation of new insights in the public sector</span></span></p></li><li><p><span><span>creation of a new sector adding value to the economy</span></span></p></li><li><p><span><span>creation of new data based on combining data</span></span></p></li><li><p><span><span>visibility and publicity for the data provider</span></span></p></li><li><p><span><span>improvement of citizen services</span></span></p></li></ul><p><span><span>This category of benefits varies greatly based on the type of data being opened up.</span></span></p><p><span><span>In addition, opening the data might create some </span><strong>monetization </strong><span>opportunities. For instance, a city could sell access to real-time feeds of its data (e.g. to hedge funds, insurance companies) while making the exact same data publicly accessible but delayed on its portal within a week.</span></span></p><p><span><em>D for duty</em></span></p><p><span><span>(D) stands for Duty in the original paper. In our setting it translates more into “ecosystem impact” or “global impact”, and is industry specific. This represents the positive impact of opening the data for other players.</span></span></p><p><span><span>Public entities will see the value of opening the data in terms of </span><strong>better governance (</strong><span>transparency, democratic accountability, collaboration, participation), </span><strong>improved quality-of-life for citizens</strong><span>, </span><strong>better government-to-government interactions, equal access to data </strong><span>and </span><strong>better economic development.</strong></span></p><p><span><span>Private entities will see the value more in terms of </span><strong>corporate social responsibility.</strong></span></p><p><span><span>End users will see value in terms of </span><strong>social responsibility </strong><span>and </span><a href="http://en.wikipedia.org/wiki/Prosocial_behavior"><span>pro-social behavior</span></a><span>.</span></span></p><p><span><em>C for costs</em></span></p><p><span><span>Finally (C) stands for cost which is influenced by the following factors.</span></span></p><ul><li><p><span><strong>Opening costs</strong><span>,</span><strong> </strong><span>i.e. the costs of opening the data itself. These costs involve the costs of transitioning data buried inside legacy systems and reformatting the data into an open format.</span></span></p></li><li><p><span><strong>Operating costs</strong><span>, i.e. the cost of publishing the data and keeping it fresh. Even with affordable commercial offerings and open source solutions, there is still a cost in running an open data portal.</span></span></p></li><li><p><span><strong>Quality costs</strong><span>,</span><strong> </strong><span>i.e. costs of keeping the data fresh.</span></span></p></li><li><p><span><strong>Legal costs</strong><span>,</span><strong> </strong><span>i.e. costs of opening the data in compliance with the various legislations. Finding legal expertise in this relatively new field can be hard to find and therefore expensive; this is even worse when dealing with multiple legal jurisdictions lacking harmonization (e.g. Europe vs. U.S.).</span></span></p></li><li><p><span><strong>Liability costs and risks</strong><span>,</span><strong> </strong><span>i.e. the costs when something goes wrong such as privacy, erroneous data, stale data. Again, the lack of legal clarity makes this risk harder to quantify.</span></span></p></li><li><p><span><strong>Mandatory costs</strong><span>,</span><strong> </strong><span>i.e. when opening the data is a legal requirement (resp. a right) and not doing so results in fines (resp. cost, e.g. FOIA in the U.S.).</span></span></p></li><li><p><span><strong>Competitive cost (for corporations)</strong><span>,</span><strong> </strong><span>i.e. the cost of sharing information that can be used by the competition.</span></span></p></li><li><p><span><strong>Privacy cost (for individuals)</strong><span>,</span><strong> </strong><span>i.e. the cost of sharing information that can be used to reduce quality of life, e.g. spam, insurance premium, etc.</span></span></p></li><li><p><span><strong>Public relations costs</strong><span>,</span><strong> </strong><span>i.e. the cost incurred from bad press due to information gleaned from the data, e.g. performance metrics for a city, environment metrics or workforce diversity for a corporation.</span></span></p></li><li><p><span><strong>Opportunity costs</strong><span>,</span><strong> </strong><span>because the same resources (money, technical infrastructure, human resources) could be spent doing something else.</span></span></p></li></ul><p><span><span>Particular categories of costs (e.g. transition, privacy, opportunity) will vary based on each industry.</span></span></p><p><span><strong>Pulling the levers</strong></span></p><p><span><span>The equation describes a quantity that should be greater than zero for opening the data to make sense. In some practical situations, some variables may be beyond the control of the data owner. The equation provides guidance in terms of what levers can be used and points to additional decisions to be considered.</span></span></p><p><span><strong>Focus on P: increase probability of benefits</strong></span></p><ul><li><p><span><span>Have we invested in the right people, culture to move this initiative forward?</span></span></p></li><li><p><span><span>How usable is the data for a hacker community to build off of?</span></span></p></li></ul><p><span><strong>Focus on B: increase benefits from the data</strong></span></p><ul><li><p><span><span>Is there an easy mechanism for users of the data to provide some feedback?</span></span></p></li><li><p><span><span>How does the data interact with related datasets and systems (e.g. interoperability)?</span></span></p></li></ul><p><span><strong>Focus on D: value duty and ecosystem impact</strong></span></p><ul><li><p><span><span>What’s the potential value chain impact from opening up the data? Who benefits from opening up this data?</span></span></p></li><li><p><span><span>What relationships or goodwill can we forge by making this decision?</span></span></p></li></ul><p><span><strong>Focus on C: reduce costs</strong></span></p><ul><li><p><span><span>What are the real costs in transitioning and reformatting data into a usable format?</span></span></p></li><li><p><span><span>What are the maintenance costs associated with opening the data?</span></span></p></li></ul><p><span><span>The nature of the equation involves weighing a combination of approaches increasing P, B and D while decreasing C. The following examples illustrate these approaches in action toward more optimal sharing.</span></span></p><p><span><span>Revisiting 3 examples</span></span></p><p><span><span>We revisit 3 typical use cases for open data and see how the calculus can help identify the levers that can be used to improve the outcome.</span></span></p><p><span><em>Example 1: API</em></span></p><p><span><span>Cities (and governments more generally) often start their open data efforts by simply offering raw access to their bulk datasets and only consider API as an after thought. That’s often an </span><a href="http://opensource.com/government/14/12/open-data-portals-api-first"><span>oversight</span></a><span>.</span></span></p><p><span><span>APIs force the use of standards (P↑); facilitate the organization of prize and challenges (P↑).</span></span></p><p><span><span>APIs are by nature hacker-friendly (P↑). APIs are also the natural conduit for data monetization (P↑).</span></span></p><p><span><span>APIs can be expensive to create and maintain (C↑); but they also provide a better data granularity which helps with privacy and can reduce legal liabilities (C↓).</span></span></p><p><span><span>So overall, APIs usually look like a good value proposition. Access to affordable tools and pre-existing standards can reduce the cost even more and make this option a no-brainer.</span></span></p><p><span><em>Example 2: City Data Portals</em></span></p><p><span><span>The city data portal is the web presence where a city decides to publish the datasets it is opening.</span></span></p><p><span><span>First, running such a portal costs money in terms of storage and bandwidth (C↑) . If the portal permits to run complicated queries against the data, a computation cost gets introduced (C↑). If the portal contains some social features like a forum, a community manager is needed (C↑) to handle people’s request and handle communication issues.</span></span></p><p><span><span>A good data portal will make the data easier to discover for end users (P↑).</span></span></p><p><span><span>User feedback will improve the quality of the data (B↑) and increase user engagement (P↑, D↑).</span></span></p><p><span><span>A good data portal will also benefit city agencies who can discover and leverage each other’s data (D↑). Proactively opening the data is also a good way to avoid numerous and expensive FOIA requests (C↓).</span></span></p><p><span><span>Given the availability of good data portal tools (e.g. </span><a href="http://socrata.com/"><span>Socrata</span></a><span>, </span><a href="http://ckan.org/"><span>CKAN</span></a><span>, </span><a href="http://github.com/"><span>Github</span></a><span>) and cheap hosting, the cost C is often low and a City Data Portal is usually a good option.</span></span></p><p><span><em>Example 3: Citizen Digital Philanthropy</em></span></p><p><span><span>User data can be very useful for research purposes for instance in the medical field or urban planning field. As a civic minded user, I am interested in donating my personal data. P, B and D are already high. But so is C.</span></span></p><p><span><span>In most cases, my data is actually locked at/by some provider which makes it hard to share in the first place (C↑). Also, there is little guarantee about the respect of my privacy (C↑). There is also a risk that the data I open will not be used for the exact purpose I have in mind which is captured in our equations by a lower benefit (B↓).</span></span></p><p><span><span>In this use case, the critical element seems to be cost. The existence of data clearinghouses that can anonymize data and guarantee the appropriate use of the data (per the user’s request) would make such form of philanthropy possible to user by reducing C. The use case also requires an environment where the user is actually in control of her data.</span></span></p><p><span><strong>Conclusion</strong></span></p><p><span><span>A simple equation will not answer all the questions about open data. Despite all its limitations, we think that this “calculus” can be useful as a way of anchoring the conversation, similar to </span><a href="https://medium.com/@antheaws"><span>Anthea Watson Strong</span></a><span>'s </span><a href="https://medium.com/thelist/the-three-levers-of-civic-engagement-cde106b68523"><span>The Three Levers of Civic Engagement</span></a><span>.</span></span></p><p><span><span>Looking at the formula, decision makers can see how a given factor influences the outcome. Internally, the formulation could form the logic basis for a performance measurement and decision making tool. Externally, this could be extremely valuable for governments trying to engage the private sector in sharing private sector data or for the tech community in identifying technologies that would reduce cost or amplify benefits.</span></span></p><p><span><span>By just looking at the levers, one can reasonably anticipate that (1) data marketplaces where corporations can exchange data, (2) trusted 3rd parties offering aggregation and anonymization of end user data and (3) templates — both legal and database schemas — incorporated in software portal solutions would make decisions to open data more feasible and rational for data providers.</span></span></p><p><span><span>We hope that our "calculus" for open data can better frame the question of opening the data, help identify the various levers that can be pulled and facilitate conversations and research on this topic at all levels.</span></span></p><p><span><span>Some suggested reading</span></span></p><p><span><span>For the more avid readers, here are some selected readings on the topic.</span></span></p><p><span><span>Academic research</span></span></p><ul><li><p><span><a href="http://journals.cambridge.org/abstract_S0003055400000125"><span>A Theory of the Calculus of Voting</span></a><span>, by Riker and Ordeshook, 1968.</span></span></p></li><li><p><span><a href="http://dx.doi.org/10.1080/10580530.2012.716740"><span>Benefits, Adoption Barriers and Myths of Open Data and Open Government</span></a><span>, Janssen et al., 2012.</span></span></p></li><li><p><span><a href="http://www.scielo.cl/scielo.php?pid=S0718-18762014000200001&amp;script=sci_arttext&amp;tlng=e"><span>Innovation through Open Data</span></a><span>, </span><a href="http://paperpile.com/b/QvAIzU/oX24"><span>Zuiderwijk et al., 2014</span></a><span>.</span></span></p></li><li><p><span><a href="http://link.springer.com/chapter/10.1007/978-3-662-44426-9_21"><span>A Decision Model for Data Sharing</span></a><span>, Eckartz et al., 2014.</span></span></p></li></ul><p><span><span>Impact &amp; opportunities</span></span></p><ul><li><p><span><a href="https://opengovdata.io/"><span>Open government data</span></a><span>, J. Tauberer, 2012.</span></span></p></li><li><p><span><a href="http://www.mckinsey.com/insights/business_technology/open_data_unlocking_innovation_and_performance_with_liquid_information"><span>Open data: Unlocking innovation and performance with liquid information</span></a><span>. McKinsey, 2013.</span></span></p></li><li><p><span><a href="http://www.opendatanow.com/"><span>Open Data Now</span></a><em>, </em><span>J. Gurin, 2014.</span></span></p></li><li><p><span><a href="https://www.scribd.com/doc/219477511/The-Impacts-of-Open-Data"><span>The Impacts of Open Data</span></a><span>, Sunlight Foundation, 2014.</span></span></p></li><li><p><span><a href="http://unglobalpulse.org/mapping-corporate-data-sharing"><span>Mapping the Next Frontier of Open Data: Corporate Data Sharing</span></a><span>, S. Verhulst, 2014.</span></span></p></li><li><p><span><a href="http://www.oreilly.com/data/free/data-driven.csp"><span>Data Driven: Creating a Data Culture</span></a><span>, DJ Patil and Hilary Mason, 2015.</span></span></p></li></ul><p><span><span>Data for public good</span></span></p><ul><li><p><span><a href="http://www.unglobalpulse.org/data-philanthropy-where-are-we-now"><span>Data philanthropy: Where are we now</span></a><span>, </span><em>UN Global Pulse Blog</em><span>, 2013.</span></span></p></li><li><p><span><a href="http://www.cs.nott.ac.uk/~jog/papers/DataDonation.pdf"><span>Data Donation: Sharing Personal Data for Public Good?</span></a><span>, Skatova et al., 2014.</span></span></p></li><li><p><span><a href="http://nextcity.org/daily/entry/traffic-data-privacy-sharing-city-planning"><span>Would You Share Private Data for the Good of City Planning?</span></a><span>, H. Grabar, 2015.</span></span></p></li><li><p><span><a href="http://www.scientificamerican.com/article/donated-personal-data-could-aid-lifestyle-researchers/"><span>Donated Personal Data Could Aid Lifestyle Researchers</span></a><span>, Skatova et al., 2015.</span></span></p></li><li><p><span><span>“I Quant NY” at </span><a href="http://iquantny.tumblr.com/"><span>http://iquantny.tumblr.com</span></a></span></p></li></ul><p><span><span>Stay tuned for a forthcoming version of this work.</span></span></p>]]></description></item><item><title>R-Search – Rapid Re-Search Enabling the Design of Agile and Creative Responses to Problems</title><category>Blog</category><dc:creator>Andrew Young</dc:creator><pubDate>Thu, 26 Feb 2015 21:15:41 +0000</pubDate><link>http://andrew-young-513z.squarespace.com/blog/2015/2/26/impact-evaluation-strategies-for-open-data-research-projects</link><guid isPermaLink="false">54ef86f0e4b060284dcf9f23:54ef8cdce4b0819f8ee6fa11:54ef8cece4b07600a0a4d75f</guid><description><![CDATA[<p><em>By Stefaan Verhulst and Andrew Young</em></p><p><span><span>How to quickly, yet systematically, become smart about a topic when seeking solutions to public problems? What questions to answer that can help assess the problem and solutions space better? How to identify rapidly the stakeholders that matter? Today, the GovLab released a new resource seeking to enable problem-solvers to design agile and creative responses to public challenges based upon research and due diligence.</span></span></p><p><span><span>The “R-Search” rapid research methodology encompasses two central components that, when leveraged, can enable researchers to develop, iterate and implement evidence-based solutions to public problems:</span></span></p><ul><li><p><span><span>&nbsp;Getting smart quickly on a topic by:</span></span></p><ul><li><p><span><span>developing a clear and detailed understanding of the problem and solution area;</span></span></p></li><li><p><span><span>identifying actors at play in the problem and solution space; and</span></span></p></li><li><p><span><span>understanding the larger context in which the problem and potential solutions exist.</span></span></p></li></ul></li><li><p><span><span>Staying in the know regarding new developments in the problem and solution space.</span></span></p></li></ul><p><span><span>R-Search, consistent with the GovLab’s action research approach, is not premised on the belief that static, in-the-lab research should get in the way of real-world problem-solving. Rather, the R-Search approach is built around the notion that:</span></span></p><ul><li><p><span><span>Seemingly intractable problems require agile and creative responses; and</span></span></p></li><li><p><span><span>Meaningful and agile creativity can only arise when there is a rapid understanding of the topic at hand.</span></span></p></li></ul><p><span><span>R-Search, therefore, seeks to:</span></span></p><ul><li><p><span><span>enable the development of a MAP (a topic’s </span><strong>M</strong><span>ilieu, relevant </span><strong>A</strong><span>ctors and existing </span><strong>P</strong><span>roblem space) of issues, scholarship, actions and opinions surrounding a topic to allow for the design of responses that are more informed and targeted;</span></span></p></li><li><p><span><span>allow for the development of a baseline against which progress can be measured;</span></span></p></li><li><p><span><span>enable the completion of a project canvas to guide development, implementation and assessment; and</span></span></p></li><li><p><span><span>provide for knowledge-building to inform rapid prototyping.</span></span></p></li></ul><p><span><a href="http://images.thegovlab.org/wordpress/wp-content/uploads/2015/01/R-Search.pdf"><span>View the full resource here</span></a><span>.</span></span></p>]]></description></item></channel></rss>